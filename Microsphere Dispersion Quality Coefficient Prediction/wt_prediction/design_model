import pickle
from adjustText import adjust_text
from scipy.interpolate import interp1d
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import Ridge, Lasso
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
import joblib
import os
import warnings

# === 忽略不必要的警告 ===
warnings.filterwarnings("ignore")

# ================= 1. 动态配置相对路径 =================
# 获取当前脚本所在的目录
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# 定义数据集文件夹路径 (在脚本路径下的 "数据集" 文件夹)
DATASET_DIR = os.path.join(BASE_DIR, 'DataSet')

# 确保数据集文件夹存在
if not os.path.exists(DATASET_DIR):
    os.makedirs(DATASET_DIR)
    print(f"警告: '{DATASET_DIR}' 文件夹不存在，已自动创建。请将 d1.csv 放入其中。")

# 输入文件路径
INPUT_PATH = os.path.join(DATASET_DIR, 'd1.csv')

# 输出文件路径
OUTPUT_CSV_PATH = os.path.join(DATASET_DIR, 'Predicted_vs_Actual_wt_data.csv')
MODEL_SAVE_DIR = DATASET_DIR  # 模型也保存在数据集文件夹中

# 可视化图片保存路径 (保存在脚本同级目录下)
PREDICTION_PLOT_PATH = os.path.join(BASE_DIR, 'prediction_result.png')
LEARNING_CURVE_PATH = os.path.join(BASE_DIR, 'learning_curve.png')

print(f"工作目录: {BASE_DIR}")
print(f"数据目录: {DATASET_DIR}")
# ==========================================================

# 设置 matplotlib 中文显示
plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号


def train_models():
    # 加载数据
    print(f"正在读取数据: {INPUT_PATH}")
    if not os.path.exists(INPUT_PATH):
        print(f"错误: 找不到文件 {INPUT_PATH}")
        print("请确保 'd1.csv' 位于脚本同级目录的 '数据集' 文件夹中。")
        exit()

    try:
        data = pd.read_csv(INPUT_PATH, encoding='ISO-8859-1', sep=',', header=None)
    except Exception as e:
        print(f"读取 CSV 失败: {e}")
        exit()

    # 适配 4 列数据
    data.columns = ['size', 'cv', 'wt%', 'level']

    # print("数据加载成功，前5行预览:")
    # print(data.head())

    data['size'] = data['size'] + 1  # 避免潜在的数值问题

    # 根据 level 和 cv 分组
    data_1 = data[(data['level'] <= 1.5) & (data['cv'] <= 3)]
    data_234 = data[(data['level'] > 1.5) & (data['cv'] <= 3)]

    # 准备数据集
    X_1 = data_1[['size']]
    X_234 = data_234[['size']]
    y_1 = data_1['wt%']
    y_234 = data_234['wt%']

    # 标准化
    scaler = StandardScaler()
    X_1_scaled = scaler.fit_transform(X_1)
    X_234_scaled = scaler.fit_transform(X_234)

    # 创建模型流水线
    pipeline_234 = make_pipeline(PolynomialFeatures(degree=2), Ridge())
    pipeline_1 = make_pipeline(PolynomialFeatures(degree=3), Lasso())

    # 定义参数搜索范围
    param_grid_234 = {'ridge__alpha': np.logspace(-5, 5, 7)}
    param_grid_1 = {'lasso__alpha': np.logspace(-5, 5, 7)}

    # 动态调整 KFold 的 n_splits
    n_splits_234 = min(8, len(X_234)) if len(X_234) > 1 else 1
    n_splits_1 = min(8, len(X_1)) if len(X_1) > 1 else 1

    # 训练模型 234
    if n_splits_234 > 1:
        cv_234 = KFold(n_splits=n_splits_234, shuffle=True, random_state=42)
        grid_search_234 = GridSearchCV(pipeline_234, param_grid_234, cv=cv_234, scoring='neg_mean_squared_error',
                                       n_jobs=1)
        grid_search_234.fit(X_234_scaled, y_234)
        best_alpha_234 = grid_search_234.best_params_['ridge__alpha']
    else:
        best_alpha_234 = 1.0

    # 训练模型 1
    if n_splits_1 > 1:
        cv_1 = KFold(n_splits=n_splits_1, shuffle=True, random_state=42)
        grid_search_1 = GridSearchCV(pipeline_1, param_grid_1, cv=cv_1, scoring='neg_mean_squared_error', n_jobs=1)
        grid_search_1.fit(X_1_scaled, y_1)
        best_alpha_1 = grid_search_1.best_params_['lasso__alpha']
    else:
        best_alpha_1 = 1.0

    # 使用最佳参数重新构建模型
    model_234 = make_pipeline(PolynomialFeatures(degree=2), Ridge(alpha=best_alpha_234))
    model_1 = make_pipeline(PolynomialFeatures(degree=3), Lasso(alpha=best_alpha_1))

    model_234.fit(X_234_scaled, y_234)

    # 在训练模型时添加梯度特征
    gradients, second_derivatives = compute_gradients(model_234, X_1_scaled)
    # 扩展特征
    X_1_extended_with_gradient = np.hstack((X_1_scaled, gradients.reshape(-1, 1)))
    model_1.fit(X_1_extended_with_gradient, y_1)

    # 保存模型
    joblib.dump(model_234, os.path.join(MODEL_SAVE_DIR, 'model_234.pkl'))
    joblib.dump(model_1, os.path.join(MODEL_SAVE_DIR, 'model_1.pkl'))
    joblib.dump(scaler, os.path.join(MODEL_SAVE_DIR, 'scaler.pkl'))

    print("模型和标准化器已成功保存。")

    # 返回 cv 对象
    return_cv = cv_234 if 'cv_234' in locals() else None
    return scaler, model_234, model_1, data, best_alpha_234, best_alpha_1, X_1_scaled, return_cv


# 定义计算梯度（斜率）的函数
def compute_gradients(model, X_scaled):
    epsilon = 1e-5
    predictions = model.predict(X_scaled)
    gradients = []
    second_derivatives = []
    for i in range(len(X_scaled)):
        X_perturbed = X_scaled.copy()
        X_perturbed[i] += epsilon
        prediction_perturbed = model.predict(X_perturbed)
        gradient = (prediction_perturbed[i] - predictions[i]) / epsilon
        gradients.append(gradient)

        X_perturbed2 = X_scaled.copy()
        X_perturbed2[i] -= epsilon
        prediction_perturbed2 = model.predict(X_perturbed2)
        second_derivative = (prediction_perturbed[i] - prediction_perturbed2[i]) / (2 * epsilon)
        second_derivatives.append(second_derivative)
    return np.array(gradients), np.array(second_derivatives)


# 动态调整预测值
def adjust_predictions_with_gradient(model_234, model_1, X_scaled, y_pred_1):
    gradients, second_derivatives = compute_gradients(model_234, X_scaled)
    adjusted_predictions = []
    for i in range(len(y_pred_1)):
        adjustment = 0.0
        if second_derivatives[i] > 0:
            adjustment = abs(gradients[i]) * 0.01
        elif second_derivatives[i] < 0:
            adjustment = abs(gradients[i]) * 0.005
        else:
            adjustment = abs(gradients[i]) * 0.005

        if gradients[i] > 0:
            adjusted_predictions.append(y_pred_1[i] + adjustment)
        elif gradients[i] < 0:
            adjusted_predictions.append(y_pred_1[i] - adjustment)
        else:
            adjusted_predictions.append(y_pred_1[i])
    return np.array(adjusted_predictions)


def evaluate_models(scaler, model_234, model_1, data):
    # 使用 CV 筛选后的数据进行评估
    X_1_scaled = scaler.transform(data[(data['level'] <= 1.5) & (data['cv'] <= 3)][['size']])
    y_1 = data[(data['level'] <= 1.5) & (data['cv'] <= 3)]['wt%']

    X_234_scaled = scaler.transform(data[(data['level'] > 1.5) & (data['cv'] <= 3)][['size']])
    y_234 = data[(data['level'] > 1.5) & (data['cv'] <= 3)]['wt%']

    y_234_pred = model_234.predict(X_234_scaled)
    mse_234 = mean_squared_error(y_234, y_234_pred)
    r2_234 = r2_score(y_234, y_234_pred)

    gradients, _ = compute_gradients(model_234, X_1_scaled)
    X_1_extended_with_gradient = np.hstack((X_1_scaled, gradients.reshape(-1, 1)))

    y_1_pred = model_1.predict(X_1_extended_with_gradient)
    mse_1 = mean_squared_error(y_1, y_1_pred)
    r2_1 = r2_score(y_1, y_1_pred)

    print("Model 234 - MSE:", mse_234, "R2 Score:", r2_234)
    print("Model 1 - MSE:", mse_1, "R2 Score:", r2_1)

    n_splits = min(5, len(X_234_scaled))

    if n_splits > 1:
        cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)
        cv_scores_234 = cross_val_score(model_234, X_234_scaled, y_234, cv=cv, scoring='neg_mean_squared_error',
                                        n_jobs=1)
        cv_scores_1 = cross_val_score(model_1, X_1_extended_with_gradient, y_1, cv=cv, scoring='neg_mean_squared_error',
                                      n_jobs=1)
        print("Model 234 - CV MSE:", -np.mean(cv_scores_234))
        print("Model 1 - CV MSE:", -np.mean(cv_scores_1))
        return X_234_scaled, X_1_extended_with_gradient, y_234, y_1, cv
    else:
        print("样本量过少，无法进行交叉验证")
        return X_234_scaled, X_1_extended_with_gradient, y_234, y_1, None


def process_and_save_predictions(model_234, model_1, scaler, data, X_1_scaled):
    new_sizes = data['size'].unique().reshape(-1, 1)

    new_sizes_scaled = scaler.transform(new_sizes)
    gradients = compute_gradients(model_234, new_sizes_scaled)[0]

    new_features = np.hstack((new_sizes_scaled, gradients.reshape(-1, 1)))
    new_predictions = model_1.predict(new_features)

    # 获取实际数据
    actual_data = data[data['level'].isin([1, 1.5]) & data['size'].isin(new_sizes.flatten())]

    predictions_df = pd.DataFrame({'size': new_sizes.flatten(), 'predicted_wt%': new_predictions})
    comparison_df = pd.merge(predictions_df, actual_data[['size', 'wt%', 'level']], on='size', how='left')
    comparison_df.rename(columns={'wt%': 'actual_wt%'}, inplace=True)

    extracted_data = comparison_df[['size', 'predicted_wt%', 'actual_wt%', 'level']].copy()
    extracted_data.rename(columns={
        'size': 'Size',
        'predicted_wt%': 'Predicted wt%',
        'actual_wt%': 'Actual wt%',
        'level': 'Level'
    }, inplace=True)

    extracted_data.to_csv(OUTPUT_CSV_PATH, index=False)
    print(f"数据已成功保存至 {OUTPUT_CSV_PATH}")

    matching_sizes = comparison_df['size'].values.reshape(-1, 1)
    matching_sizes_scaled = scaler.transform(matching_sizes)

    gradients, _ = compute_gradients(model_234, matching_sizes_scaled)
    matching_features = np.hstack((matching_sizes_scaled, gradients.reshape(-1, 1)))
    matching_predictions = model_1.predict(matching_features)

    global matching_adjusted_predictions
    matching_adjusted_predictions = adjust_predictions_with_gradient(
        model_234, model_1, matching_sizes_scaled, matching_predictions
    )

    pkl_path = os.path.join(MODEL_SAVE_DIR, "matching_predictions.pkl")
    with open(pkl_path, "wb") as f:
        pickle.dump(matching_adjusted_predictions, f)
    print(f"预测结果已保存至 {pkl_path}")

    comparison_df = comparison_df.drop_duplicates(subset=['size'])
    matching_adjusted_predictions = matching_adjusted_predictions[:len(comparison_df)]

    plt.figure(figsize=(12, 8))
    plt.scatter(comparison_df['size'], comparison_df['actual_wt%'], color='red', label='实际 wt%', alpha=0.6)
    plt.plot(comparison_df['size'], matching_adjusted_predictions, label='调整后预测 wt%', color='blue')
    texts = []
    for i in range(len(comparison_df)):
        size = comparison_df.iloc[i]['size']
        actual_wt = comparison_df.iloc[i]['actual_wt%']
        predicted_wt = matching_adjusted_predictions[i]
        if not np.isnan(actual_wt):
            text_actual = plt.text(size, actual_wt, f"({size:.0f}, {actual_wt:.4f})", fontsize=8, color='red',
                                   ha='right')
            texts.append(text_actual)
        text_predicted = plt.text(size, predicted_wt, f"({size:.0f}, {predicted_wt:.4f})", fontsize=8, color='blue',
                                  ha='left')
        texts.append(text_predicted)

    try:
        adjust_text(texts, arrowprops=dict(arrowstyle='->', color='gray', lw=0.5))
    except Exception as e:
        print(f"警告: adjust_text 绘图优化失败，将使用默认位置。错误: {e}")

    plt.xlabel('Size')
    plt.ylabel('wt%')
    plt.legend()
    plt.title('调整后的模型 1 - 预测 vs 实际 wt%')

    # === 优化：保存图片而不是仅仅展示 ===
    plt.savefig(PREDICTION_PLOT_PATH)
    print(f"预测对比图已保存至: {PREDICTION_PLOT_PATH}")
    # plt.show() # 如果不需要弹出窗口，可以注释掉


def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None):
    if cv is None:
        return
    try:
        train_sizes, train_scores, test_scores = learning_curve(
            estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5))

        train_scores_mean = np.mean(train_scores, axis=1)
        train_scores_std = np.std(train_scores, axis=1)
        test_scores_mean = np.mean(test_scores, axis=1)
        test_scores_std = np.std(test_scores, axis=1)

        plt.figure()
        plt.title(title)
        if ylim is not None:
            plt.ylim(*ylim)
        plt.xlabel("训练样本数")
        plt.ylabel("得分 (Score)")
        plt.grid()

        plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                         train_scores_mean + train_scores_std, alpha=0.1, color="r")
        plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                         test_scores_mean + test_scores_std, alpha=0.1, color="g")
        plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="训练集得分")
        plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="交叉验证得分")

        plt.legend(loc="best")

        # === 优化：保存学习曲线图片 ===
        # 根据标题区分保存文件名
        safe_title = title.replace(" ", "_")
        save_path = os.path.join(BASE_DIR, f"{safe_title}.png")
        plt.savefig(save_path)
        print(f"学习曲线已保存至: {save_path}")
        # plt.show() # 如果不需要弹出窗口，可以注释掉

    except Exception as e:
        print(f"警告: 学习曲线绘制失败: {e}")


def main():
    scaler, model_234, model_1, data, best_alpha_234, best_alpha_1, X_1_scaled, cv = train_models()
    evaluate_models(scaler, model_234, model_1, data)
    process_and_save_predictions(model_234, model_1, scaler, data, X_1_scaled)
    X_234_scaled, X_1_extended_with_gradient, y_234, y_1, cv = evaluate_models(scaler, model_234, model_1, data)

    # 绘制学习曲线
    plot_learning_curve(model_234, "Model 234 Learning curve", X_234_scaled, y_234, cv=cv)
    plot_learning_curve(model_1, "Model 1 Learning curve", X_1_extended_with_gradient, y_1, cv=cv)


if __name__ == "__main__":
    main()
